ckpt = '/media/yueyulin/KINGSTON/models/rwkv6/RWKV-x060-World-1B6-v2.1-20240328-ctx4096.pth'
states_file = '/tmp/states.pth'
tokenizer_file = '/home/yueyulin/github/RWKV_LM_EXT/tokenizer/rwkv_vocab_v20230424.txt'

import os
parent_path = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
import sys
sys.path.append(parent_path)
print(f'add path: {parent_path} to sys.path')
os.environ['RWKV_JIT_ON'] = '0'
os.environ['RWKV_T_MAX'] = '4096'
os.environ['RWKV_FLOAT_MODE'] = 'bf16'
os.environ['RWKV_HEAD_SIZE_A'] = '64'
os.environ['RWKV_T_MAX'] = '4096'
os.environ["RWKV_MY_TESTING"]='x060'
os.environ['RWKV_CTXLEN'] = '4096'
import torch
from src.model_run import RWKV,PIPELINE_ARGS,create_empty_args,load_embedding_ckpt_and_parse_args,generate
device = 'cuda'
dtype = torch.bfloat16
args = create_empty_args()
w = load_embedding_ckpt_and_parse_args(ckpt, args)
print(args)
model = RWKV(args)
info = model.load_state_dict(w)
model.eval()
print(model)
print(info)

states = torch.load(states_file)
print(states.keys())
states_value = []
n_head = args.n_head
head_size = args.head_size_a
for i in range(args.n_layer):
    key = f'blocks.{i}.att.time_state'
    print(key)
    value = states[key]
    prev_x = torch.zeros(args.n_embd,device=device,dtype=torch.float)#n_embd 2048 
    prev_states = torch.tensor(value,device=device,dtype=torch.float)#n_head,head_size,head_size 32,64,64
    prev_ffn = torch.zeros(args.n_embd,device=device,dtype=torch.float)#n_embd 2048 
    print(prev_x.shape)
    print(prev_states.shape)
    print(prev_ffn.shape)
    states_value.append(prev_x)
    states_value.append(prev_states)
    states_value.append(prev_ffn)
gen_args = PIPELINE_ARGS(temperature = 1.0, top_p = 0.8, top_k = 100, # top_k = 0 then ignore
                        alpha_frequency = 0.25,
                        alpha_presence = 0.25,
                        alpha_decay = 0.996, # gradually decay the penalty
                        token_ban = [], # ban the generation of some tokens
                        token_stop = [0,1], # stop generation whenever you see any token here
                        chunk_len = 256)
cat_char = 'ğŸ±'
bot_char = 'ğŸ¤–'
instruction ='æ ¹æ®ç»™å®šçš„çŸ­æ–‡ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼šåŠ¨ç‰©çš„å™¨å®˜æ„Ÿè§‰ä¸äººçš„ç›¸æ¯”æœ‰ä»€ä¹ˆä¸åŒ?'
input_text = 'è®¸å¤šåŠ¨ç‰©çš„æŸäº›å™¨å®˜æ„Ÿè§‰ç‰¹åˆ«çµæ•ï¼Œå®ƒä»¬èƒ½æ¯”äººç±»æå‰çŸ¥é“ä¸€äº›ç¾å®³äº‹ä»¶çš„å‘ç”Ÿï¼Œä¾‹å¦‚ï¼Œæµ·æ´‹ä¸­çš„æ°´æ¯èƒ½é¢„æŠ¥é£æš´ï¼Œè€é¼ èƒ½äº‹å…ˆèº²é¿çŸ¿äº•å´©å¡Œæˆ–æœ‰å®³æ°”ä½“ï¼Œç­‰ç­‰ã€‚åœ°éœ‡å¾€å¾€èƒ½ä½¿ä¸€äº›åŠ¨ç‰©çš„æŸäº›æ„Ÿè§‰å™¨å®˜å—åˆ°åˆºæ¿€è€Œå‘ç”Ÿå¼‚å¸¸ååº”ã€‚å¦‚ä¸€ä¸ªåœ°åŒºçš„é‡åŠ›å‘ç”Ÿå˜å¼‚ï¼ŒæŸäº›åŠ¨ç‰©å¯èƒ½é€šè¿‡å®ƒä»¬çš„å¹³è¡¡å™¨å®˜æ„Ÿè§‰åˆ°ï¼›ä¸€ç§æŒ¯åŠ¨å¼‚å¸¸ï¼ŒæŸäº›åŠ¨ç‰©çš„å¬è§‰å™¨å®˜ä¹Ÿè®¸èƒ½å¤Ÿå¯Ÿè§‰å‡ºæ¥ã€‚åœ°éœ‡å‰åœ°ä¸‹å²©å±‚æ—©å·²åœ¨é€æ—¥ç¼“æ…¢æ´»åŠ¨ï¼Œè€Œæ–­å±‚é¢ä¹‹é—´åˆå…·æœ‰å¼ºå¤§çš„æ‘©æ“¦åŠ›ã€‚è¿™ç§æ‘©æ“¦åŠ›ä¼šäº§ç”Ÿä¸€ç§ä½äºäººçš„å¬è§‰æ‰€èƒ½æ„Ÿè§‰åˆ°çš„ä½é¢‘å£°æ³¢ã€‚äººå¯¹æ¯ç§’20æ¬¡ä»¥ä¸Šçš„å£°æ³¢æ‰èƒ½æ„Ÿè§‰åˆ°ï¼Œè€ŒåŠ¨ç‰©åˆ™ä¸ç„¶ã€‚é‚£äº›æ„Ÿè§‰ååˆ†çµæ•çš„åŠ¨ç‰©ï¼Œåœ¨æ„Ÿè§¦åˆ°è¿™ç§ä½å£°æ³¢æ—¶ï¼Œä¾¿ä¼šæƒŠæä¸‡çŠ¶ï¼Œä»¥è‡³å‡ºç°å†¬è›‡å‡ºæ´ã€é±¼è·ƒæ°´é¢ç­‰å¼‚å¸¸ç°è±¡ã€‚'
ctx = f'{cat_char}:{instruction}\n{input_text}\n{bot_char}:'
print(ctx)
from tokenizer.rwkv_tokenizer import TRIE_TOKENIZER
tokenizer = TRIE_TOKENIZER(tokenizer_file)
print(tokenizer.encode(ctx))
model = model.to(dtype)
model = model.to(device)
with torch.no_grad():
    with torch.autocast(enabled=True,device_type='cuda',dtype=dtype):
        output = generate(model, ctx,tokenizer, token_count=128, args=gen_args,callback=None,state=states_value)
    print(output)
