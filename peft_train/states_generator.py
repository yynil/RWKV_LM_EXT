ckpt = '/media/yueyulin/KINGSTON/models/rwkv6/RWKV-x060-World-7B-v2.1-20240507-ctx4096.pth'

states_file = '/media/yueyulin/data_4t/models/states_tuning/large_lr/trainable_model/epoch_0/RWKV-x060-World-7B-v2.1-20240507-ctx4096.pth.pth'
tokenizer_file = '/home/yueyulin/github/RWKV_LM_EXT/tokenizer/rwkv_vocab_v20230424.txt'

import os
parent_path = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
import sys
sys.path.append(parent_path)
print(f'add path: {parent_path} to sys.path')
os.environ['RWKV_JIT_ON'] = '0'
os.environ['RWKV_T_MAX'] = '4096'
os.environ['RWKV_FLOAT_MODE'] = 'bf16'
os.environ['RWKV_HEAD_SIZE_A'] = '64'
os.environ['RWKV_T_MAX'] = '4096'
os.environ["RWKV_MY_TESTING"]='x060'
os.environ['RWKV_CTXLEN'] = '4096'
import torch
from src.model_run import RWKV,PIPELINE_ARGS,create_empty_args,load_embedding_ckpt_and_parse_args,generate
device = 'cuda'
dtype = torch.bfloat16
args = create_empty_args()
w = load_embedding_ckpt_and_parse_args(ckpt, args)
print(args)
model = RWKV(args)
info = model.load_state_dict(w)
model.eval()
print(model)
print(info)

states = torch.load(states_file)
print(states.keys())
states_value = []
n_head = args.n_head
head_size = args.head_size_a
for i in range(args.n_layer):
    key = f'blocks.{i}.att.time_state'
    print(key)
    value = states[key]
    prev_x = torch.zeros(args.n_embd,device=device,dtype=torch.float)#n_embd 2048 
    prev_states = torch.tensor(value,device=device,dtype=torch.float).transpose(1,2)#n_head,head_size,head_size 32,64,64
    prev_ffn = torch.zeros(args.n_embd,device=device,dtype=torch.float)#n_embd 2048 
    print(prev_x.shape)
    print(prev_states.shape)
    print(prev_ffn.shape)
    states_value.append(prev_x)
    states_value.append(prev_states)
    states_value.append(prev_ffn)
gen_args = PIPELINE_ARGS(temperature = 1.0, top_p = 0, top_k = 0, # top_k = 0 then ignore
                        alpha_frequency = 0.25,
                        alpha_presence = 0.25,
                        alpha_decay = 0.996, # gradually decay the penalty
                        token_ban = [], # ban the generation of some tokens
                        token_stop = [0,1], # stop generation whenever you see any token here
                        chunk_len = 512)
cat_char = 'ğŸ±'
bot_char = 'ğŸ¤–'
instruction ='ä½ æ˜¯ä¸“é—¨è¿›è¡Œå®ä½“æŠ½å–çš„ä¸“å®¶ã€‚è¯·ä»inputä¸­æŠ½å–å‡ºç¬¦åˆschemaå®šä¹‰çš„å®ä½“ï¼Œä¸å­˜åœ¨çš„å®ä½“ç±»å‹è¿”å›ç©ºåˆ—è¡¨ã€‚è¯·æŒ‰ç…§JSONå­—ç¬¦ä¸²çš„æ ¼å¼å›ç­”ã€‚'
input_text = '{\"input\":\"6 æœˆ 17 æ—¥ï¼Œå¹¿å‘è¯åˆ¸ç ”æŠ¥æŒ‡å‡ºï¼Œè¿‘æœŸå¤§é£æœºå„é¡¹è¿›å±•æŒç»­æ¨è¿›ã€‚6 æœˆ 14 æ—¥ï¼Œä¸œèˆª C919 æœºå‹å¼€å¯ç¬¬å››æ¡å•†ä¸šå®šæœŸèˆªçº¿â€”â€”ä¸Šæµ·è™¹æ¡¥å¾€è¿”å¹¿å·ç™½äº‘ã€‚\
\
å·¥ä¸šå’Œä¿¡æ¯åŒ–éƒ¨ã€å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘å§”å‘˜ä¼š 6 æœˆ 14 æ—¥ç­¾ç½²åˆä½œåè®®ï¼Œå…±åŒè®¾ç«‹å¤§é£æœºåŸºç¡€ç ”ç©¶è”åˆåŸºé‡‘ã€‚\
\
å…¨çƒç§¯å‹é£æœºè®¢å•è¶… 1.4 ä¸‡æ¶ï¼Œå½“å‰å…¨çƒèˆªç©ºä¸šå› é›¶éƒ¨ä»¶ä¾›åº”çŸ­ç¼ºã€äº¤ä»˜å‘¨æœŸå˜é•¿ç­‰é—®é¢˜é¢ä¸´ä¾›åº”é“¾å¨èƒï¼Œæˆ–ä¸ºå›½å†…èˆªç©ºèˆªå‘äº§ä¸šé“¾ç›¸å…³ä¼ä¸šå¸¦æ¥èˆªç©ºå‡ºæµ·ä¸šåŠ¡æ–°å¢é‡ã€‚\",\
\"schema\":[\"åœ°ç†ä½ç½®\",\"ç»„ç»‡æœºæ„\",\"æ°”å€™ç±»å‹\",\"æ—¶é—´\"]}'
ctx = f'{cat_char}:{instruction}\n{input_text}\n{bot_char}:'
print(ctx)
from tokenizer.rwkv_tokenizer import TRIE_TOKENIZER
tokenizer = TRIE_TOKENIZER(tokenizer_file)

model = model.to(dtype)
model = model.to(device)
with torch.no_grad():
    with torch.autocast(enabled=True,device_type='cuda',dtype=dtype):
        output = generate(model, ctx,tokenizer, token_count=128, args=gen_args,callback=None,state=states_value)
    print(output)
