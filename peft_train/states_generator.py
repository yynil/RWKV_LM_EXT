ckpt = '/media/yueyulin/KINGSTON/models/rwkv6/RWKV-x060-World-1B6-v2.1-20240328-ctx4096.pth'
states_file = '/media/yueyulin/data_4t/models/states_tuning/states_tuning/20240516-124747/trainable_model/epoch_1_step_225000/RWKV-x060-World-1B6-v2.1-20240328-ctx4096.pth.pth'
tokenizer_file = '/home/yueyulin/github/RWKV_LM_EXT/tokenizer/rwkv_vocab_v20230424.txt'

import os
parent_path = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
import sys
sys.path.append(parent_path)
print(f'add path: {parent_path} to sys.path')
os.environ['RWKV_JIT_ON'] = '0'
os.environ['RWKV_T_MAX'] = '4096'
os.environ['RWKV_FLOAT_MODE'] = 'bf16'
os.environ['RWKV_HEAD_SIZE_A'] = '64'
os.environ['RWKV_T_MAX'] = '4096'
os.environ["RWKV_MY_TESTING"]='x060'
os.environ['RWKV_CTXLEN'] = '4096'
import torch
from src.model_run import RWKV,PIPELINE_ARGS,create_empty_args,load_embedding_ckpt_and_parse_args,generate
device = 'cuda'
dtype = torch.bfloat16
args = create_empty_args()
w = load_embedding_ckpt_and_parse_args(ckpt, args)
print(args)
model = RWKV(args)
info = model.load_state_dict(w)
model.eval()
print(model)
print(info)

states = torch.load(states_file)
print(states.keys())
states_value = []
n_head = args.n_head
head_size = args.head_size_a
for i in range(args.n_layer):
    key = f'blocks.{i}.att.time_state'
    print(key)
    value = states[key]
    prev_x = torch.zeros(args.n_embd,device=device,dtype=torch.float)#n_embd 2048 
    prev_states = torch.tensor(value,device=device,dtype=torch.float)#n_head,head_size,head_size 32,64,64
    prev_ffn = torch.zeros(args.n_embd,device=device,dtype=torch.float)#n_embd 2048 
    print(prev_x.shape)
    print(prev_states.shape)
    print(prev_ffn.shape)
    states_value.append(prev_x)
    states_value.append(prev_states)
    states_value.append(prev_ffn)
gen_args = PIPELINE_ARGS(temperature = 1.0, top_p = 0.96, top_k = 20, # top_k = 0 then ignore
                        alpha_frequency = 0.25,
                        alpha_presence = 0.25,
                        alpha_decay = 0.996, # gradually decay the penalty
                        token_ban = [], # ban the generation of some tokens
                        token_stop = [0,1], # stop generation whenever you see any token here
                        chunk_len = 512)
cat_char = 'ğŸ±'
bot_char = 'ğŸ¤–'
instruction ='æ ¹æ®ç»™å®šçš„çŸ­æ–‡ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼šé»„å¾ªè´¢çš„æ˜¯å“ªå›½äººï¼Ÿ'
input_text = 'é»„å¾ªè´¢ï¼ˆè‹±è¯­ï¼šLawrence Wong Shyun Tsaiï¼Œ1972å¹´12æœˆ18æ—¥â€”ï¼‰ï¼Œæ–°åŠ å¡åè£”æ”¿æ²»äººç‰©ï¼Œç°ä»»æ–°åŠ å¡æ€»ç†å…¼è´¢æ”¿éƒ¨éƒ¨é•¿ã€äººæ°‘è¡ŒåŠ¨å…šç¤¾åŒºåŸºé‡‘ä¼šä¸»å¸­ã€‚ä»–ä¸ç‹ä¹™åº·å’Œé¢œé‡‘å‹‡å…±åŒä¸»æŒäº†å› åº”æ–°åŠ å¡2019å† çŠ¶ç—…æ¯’ç—…å¤§æµè¡Œçš„å¤šéƒ¨å§”å·¥ä½œç»„ã€‚æ›¾ä»»æ–°åŠ å¡å‰¯æ€»ç†ï¼Œæ•™è‚²éƒ¨ã€å›½å®¶å‘å±•éƒ¨ã€æ–‡åŒ–ã€ç¤¾åŒºåŠé’å¹´éƒ¨çš„éƒ¨é•¿ï¼Œé€šè®¯åŠæ–°é—»éƒ¨å’Œè´¢æ”¿éƒ¨çš„ç¬¬äºŒéƒ¨é•¿ï¼Œä»¥åŠäººæ°‘è¡ŒåŠ¨å…šå‰¯ç§˜ä¹¦é•¿ã€‚[1]é»„å¾ªè´¢æ˜¯äººæ°‘è¡ŒåŠ¨å…šç¬¬å››ä»£é¢†å¯¼å±‚ï¼Œä¹Ÿæ˜¯äººæ°‘è¡ŒåŠ¨å…šä¸­å¤®æ‰§è¡Œå§”å‘˜ä¼šé¦–ä»»å‰¯ç§˜ä¹¦é•¿å…¼æ”¿ç­–è®ºå›é¡¾é—®ã€‚'
ctx = f'{cat_char}:{instruction}\n{input_text}\n{bot_char}:'
print(ctx)
from tokenizer.rwkv_tokenizer import TRIE_TOKENIZER
tokenizer = TRIE_TOKENIZER(tokenizer_file)
print(tokenizer.encode(ctx))
model = model.to(dtype)
model = model.to(device)
with torch.no_grad():
    with torch.autocast(enabled=True,device_type='cuda',dtype=dtype):
        output = generate(model, ctx,tokenizer, token_count=128, args=gen_args,callback=None,state=states_value)
    print(output)
