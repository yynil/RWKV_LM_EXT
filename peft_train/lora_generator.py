ckpt = '/media/yueyulin/KINGSTON/models/rwkv6/RWKV-x060-World-1B6-v2.1-20240328-ctx4096.pth'
lora_file = '/media/yueyulin/data_4t/models/lora/epoch_0/RWKV-x060-World-1B6-v2.1-20240328-ctx4096.pth.pth'
tokenizer_file = '/home/yueyulin/github/RWKV_LM_EXT/tokenizer/rwkv_vocab_v20230424.txt'
target_modules = ['emb','ffn.key','ffn.value','ffn.receptance','att.key','att.value','att.receptance']
lora_r = 8
lora_alpha = 32
lora_dropout = 0
is_lora = True
import os
parent_path = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
import sys
sys.path.append(parent_path)
print(f'add path: {parent_path} to sys.path')
os.environ['RWKV_JIT_ON'] = '0'
os.environ['RWKV_T_MAX'] = '4096'
os.environ['RWKV_FLOAT_MODE'] = 'bf16'
os.environ['RWKV_HEAD_SIZE_A'] = '64'
os.environ['RWKV_T_MAX'] = '4096'
os.environ["RWKV_MY_TESTING"]='x060'
os.environ['RWKV_CTXLEN'] = '4096'
import torch
from src.model_run import RWKV,PIPELINE_ARGS,create_empty_args,load_embedding_ckpt_and_parse_args,generate
device = 'cuda'
dtype = torch.bfloat16
args = create_empty_args()
w = load_embedding_ckpt_and_parse_args(ckpt, args)
print(args)
model = RWKV(args)
info = model.load_state_dict(w)
model.eval()
print(model)
print(info)
if is_lora:
    from peft import LoraConfig
    lora_config = LoraConfig(r=lora_r,lora_alpha=lora_alpha,target_modules=target_modules,lora_dropout=lora_dropout)
    from peft import inject_adapter_in_model
    model = inject_adapter_in_model(lora_config,model,adapter_name='sft_lora')
    print(model)
    states = torch.load(lora_file)
    print(states.keys())
    info = model.load_state_dict(states,strict=False)
    print(info)
    model.eval()
states_value = None
gen_args = PIPELINE_ARGS(temperature = 1, top_p = 0.96, top_k = 20, # top_k = 0 then ignore
                        alpha_frequency = 0.25,
                        alpha_presence = 0.25,
                        alpha_decay = 0.996, # gradually decay the penalty
                        token_ban = [], # ban the generation of some tokens
                        token_stop = [0,1], # stop generation whenever you see any token here
                        chunk_len = 512)
cat_char = 'ğŸ±'
bot_char = 'ğŸ¤–'
instruction ='æ ¹æ®ç»™å®šçš„çŸ­æ–‡ï¼Œç”¨æœ€ç®€æ´çš„è¯­è¨€å›ç­”ä»¥ä¸‹é—®é¢˜ï¼šPCIeæˆä¸ºæ–°çš„ä¸ªäººç”µè„‘ä¸»æ¿æ ‡å‡†çš„æœ€ä¸»è¦çš„åŸå› æ˜¯ä»€ä¹ˆ?'
input_text = """åº”ç”¨ä¸å‰æ™¯
æŠ€å˜‰GV-NX62TC256D8æ˜¾å¡ï¼Œé‡‡ç”¨PCI Express x16æ’æ§½

åœ¨2005å¹´ï¼ŒPCIeå·²è¿‘ä¹æˆä¸ºæ–°çš„ä¸ªäººç”µè„‘ä¸»æ¿æ ‡å‡†ã€‚å…³äºæ­¤æœ‰ä¸å°‘è¯„è®ºï¼Œä½†æœ€åŸºæœ¬çš„åŸå› æ˜¯å®ƒå¯¹äºè½¯ä»¶å¼€å‘è€…å®Œå…¨é€æ˜â€”â€”ä¸ºPCIæ‰€è®¾è®¡çš„æ“ä½œç³»ç»Ÿå¯ä»¥ä¸åšä»»ä½•ä»£ç ä¿®æ”¹æ¥å¯åŠ¨PCIeè®¾å¤‡ã€‚å…¶äºŒï¼Œå®ƒèƒ½å¢å¼ºç³»ç»Ÿæ€§èƒ½ï¼Œè¿˜æœ‰å¼ºæœ‰åŠ›çš„å“ç‰Œè®¤çŸ¥ã€‚å„ç±»ç½‘å¡ã€å£°å¡ã€æ˜¾å¡ï¼Œä»¥åŠå½“ä¸‹çš„NVMeå›ºæ€ç¡¬ç›˜éƒ½ä½¿ç”¨äº†PCIeæ ‡å‡†ã€‚ä¸‹é¢ä¸ºä¸»æµçš„ä½¿ç”¨PCIe çš„å¤–è®¾äº§å“ã€‚
æ˜¾å¡

å¤§éƒ¨åˆ†æ–°å‹çš„AMDæˆ–NVIDIAæ˜¾å¡éƒ½ä½¿ç”¨PCIeæ ‡å‡†ã€‚NVIDIAåœ¨å®ƒæ–°å¼€å‘çš„SLIä¸Šé‡‡ç”¨PCIeçš„é«˜é€Ÿæ•°æ®ä¼ è¾“ï¼Œè¿™ä½¿å¾—ä¸¤å—ç›¸åŒèŠ¯ç‰‡ç»„æ˜¾å¡å¯åŒæ—¶å·¥ä½œäºä¸€å°ç”µè„‘ä¹‹ä¸Šã€‚AMDå…¬å¸ä¹ŸåŸºäºPCIeå¼€å‘ä¸€ç§ä¸¤ä¸ªGPUä¸€åŒè¿ä½œçš„æŠ€æœ¯ï¼Œç§°ä¸ºCrossFireã€‚
ç¡¬ç›˜
å½“ä¸‹ä¸»æµçš„å›ºæ€ç¡¬ç›˜æ¥å£æœ‰M.2ã€U.2ã€PCIeã€SATAã€SATA Expressã€SASç­‰ã€‚M.2å’ŒU.2å¯é€‰PCIeæ¥å£[14]ã€‚NVMeåè®®æ˜¯ç›®å‰æœ€é«˜æ•ˆçš„PCIe SSDåè®®æ ‡å‡†ã€‚  
"""
ctx = f'{cat_char}:{instruction}\n{input_text}\n{bot_char}:'
print(ctx)
from tokenizer.rwkv_tokenizer import TRIE_TOKENIZER
tokenizer = TRIE_TOKENIZER(tokenizer_file)
print(len(tokenizer.encode(ctx)))
model = model.to(dtype)
model = model.to(device)
with torch.no_grad():
    with torch.autocast(enabled=True,device_type='cuda',dtype=dtype):
        output = generate(model, ctx,tokenizer, token_count=256, args=gen_args,callback=None,state=None)
    print(output)
