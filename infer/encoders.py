import os
os.environ['RWKV_JIT_ON'] = '0'
os.environ['RWKV_T_MAX'] = '4096'
os.environ['RWKV_FLOAT_MODE'] = 'bf16'
os.environ['RWKV_HEAD_SIZE_A'] = '64'
os.environ['RWKV_T_MAX'] = '4096'
os.environ["RWKV_MY_TESTING"]='x060'
os.environ['RWKV_CTXLEN'] = '4096'
import sys
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)
print(f'add {parent_dir} to sys path')
from src.model_run import RWKV,RwkvForClassification,RwkvForSequenceEmbedding,PIPELINE_ARGS,create_empty_args,load_embedding_ckpt_and_parse_args,generate,generate_beamsearch
import torch
from torch.cuda import amp
from src.layers import inject_lora_adapter_with_state_dict,set_adapter


class BiCrossFusionEncoder:
    """
    This encoder is to fuse the bi-encoder and cross-encoder with the same rwkv base model injected with 2 sets of lora adapters.
    We have 2 assumptions here:
    1. The bi-encoder and cross-encoder share the same base model
    2. The lora types of bi-encoder and cross-encoder are the same
    This class instance is not thread-safe since we need to switch the adapter name before encoding.
    """
    def __init__(self,
                 base_rwkv,
                 bi_lora_path,
                 cross_lora_path,
                 chat_lora_path,
                 tokenizer,
                 ce_lora_r=8,
                 ce_lora_alpha=32,
                 be_lora_r=8,
                 be_lora_alpha=32,
                 chat_lora_r=8,
                 chat_lora_alpha=8,
                 target_ce_modules=['emb','ffn.key','ffn.value','ffn.receptance'],
                 target_be_modules=['emb','ffn.key','ffn.value','ffn.receptance'],
                 target_chat_modules=['att','ffn'],
                 cross_adapter_name='cross_encoder_lora',
                 bi_adapter_name='bi_embedding_lora',
                 chat_adapter_name='chat_lora',
                 sep_token_id = 2,
                 chat_pissa_path = None,
                 device = 'cuda',
                 dtype = torch.bfloat16
                 ) -> None:
        self.base_rwkv = base_rwkv
        args = create_empty_args()
        w = load_embedding_ckpt_and_parse_args(base_rwkv,args)
        rwkv = RWKV(args)
        info = rwkv.load_state_dict(w)
        print(f'load model from {base_rwkv},result is {info}')
        should_delete_head = False
        #load cross encoder and inject cross adapter
        cross_encoder_dict = torch.load(cross_lora_path,map_location='cpu')
        inject_lora_adapter_with_state_dict(
            rwkv,
            cross_adapter_name,
            cross_encoder_dict,
            ce_lora_r,
            ce_lora_alpha,
            targets=target_ce_modules,
        )
        self.cross_encoder = RwkvForClassification(rwkv,should_delete_head=should_delete_head)
        self.cross_encoder.score.weight.data = cross_encoder_dict['score.weight']
        print(f'load model from {cross_lora_path},result is {info}')
        del cross_encoder_dict
        #load bi encoder and inject bi adapter
        bi_encoder_dict = torch.load(bi_lora_path,map_location='cpu')
        inject_lora_adapter_with_state_dict(
            rwkv,
            bi_adapter_name,
            bi_encoder_dict,
            be_lora_r,
            be_lora_alpha,
            targets=target_be_modules,
        )
        add_mlp = 'dense.weight' in bi_encoder_dict
        output_dim = 0
        if add_mlp:
            output_dim = bi_encoder_dict['dense.weight'].shape[0]
        print(f'RWKV Embedding model add_mlp = {add_mlp} output_dim = {output_dim}')
        self.bi_encoder = RwkvForSequenceEmbedding(rwkv,add_mlp=add_mlp,output_dim=output_dim,should_delete_head=should_delete_head)
        if add_mlp:
            self.bi_encoder.dense.weight.data = bi_encoder_dict['dense.weight']
            self.bi_encoder.dense.bias.data = bi_encoder_dict['dense.bias']
        #load chat lora and inject chat adapter
        chat_lora_dict = torch.load(chat_lora_path,map_location='cpu')
        pissa =  torch.load(chat_pissa_path, map_location='cpu') if chat_pissa_path else None
        inject_lora_adapter_with_state_dict(
            rwkv,
            chat_adapter_name,
            chat_lora_dict,
            chat_lora_r,
            chat_lora_alpha,
            targets=target_chat_modules,
            pissa_dict=pissa
        )
        self.tokenizer = tokenizer
        self.sep_token_id = sep_token_id
        self.rwkv = rwkv
        
        #move to device
        self.cross_encoder = self.cross_encoder.to(device=device,dtype=dtype)
        self.bi_encoder = self.bi_encoder.to(device=device,dtype=dtype)
        self.rwkv = self.rwkv.to(device=device,dtype=dtype)
        self.rwkv.eval()
        self.cross_adapter_name = cross_adapter_name
        self.bi_adapter_name = bi_adapter_name
        self.chat_adapter_name = chat_adapter_name
        self.device = device
        self.dtype = dtype

    def set_adapter(self, adapter_name: str | list[str]) -> None:
        set_adapter(model=self.rwkv,adapter_name=adapter_name)

    def encode_text(self,text,chunk_size=1024):
        input_ids =  tokenizer.encode(text)
        input_ids.append(self.bi_encoder.embedding_id)
        state = None
        offset = 0
        while offset < len(input_ids):
            chunk = input_ids[offset:offset+chunk_size]
            with torch.autocast(enabled=True,device_type=self.device,dtype=self.dtype):
                outputs,state = self.bi_encoder(torch.tensor(chunk,dtype=torch.long,device=self.device),state=state)
            offset += len(chunk)

        return outputs.tolist()

    def encode_texts(self,texts):
        self.set_adapter(self.bi_adapter_name)
        return [self.encode_text(text) for text in texts]
    
    def cross_encode_text(self,text_a, text_b):
        text_a_ids = self.tokenizer.encode(text_a)
        text_b_ids = self.tokenizer.encode(text_b)
        input_ids = text_a_ids+[self.sep_token_id]+text_b_ids+[self.cross_encoder.class_id]
        offset = 0
        state = None
        with torch.autocast(enabled=True,device_type=self.device,dtype=self.dtype):
            while offset < len(input_ids):
                chunk = input_ids[offset:offset+1024]
                output,state = self.cross_encoder(torch.tensor(chunk,dtype=torch.long,device=self.device),state=state)
                offset += len(chunk)
        return output.item()

    def cross_encode_texts(self,texts_a, texts_b):
        assert len(texts_a) == len(texts_b)
        self.set_adapter(self.cross_adapter_name)
        outputs = []
        for text_a,text_b in zip(texts_a,texts_b):
            outputs.append(self.cross_encode_text(text_a,text_b))
        return outputs
    
    def beam_generate(self,instruction,input_text,token_count=128,num_beams=5,return_num_sequences=5,num_group=5,do_sample=True,is_sum_logprobs=True,length_penalty=0.6):
        self.set_adapter(self.chat_adapter_name)
        cat_char = 'ðŸ±'
        bot_char = 'ðŸ¤–'
        ctx = f'{cat_char}:{instruction}\n{input_text}\n{bot_char}:'
        with torch.no_grad():
            with torch.autocast(enabled=True,device_type=self.device,dtype=self.dtype):
                results = generate_beamsearch(
                    self.rwkv, 
                    ctx,self.tokenizer, 
                    token_count=token_count,
                    num_beams=num_beams,
                    return_num_sequences=return_num_sequences,
                    num_group=num_group,
                    do_sample=do_sample,
                    is_sum_logprobs=is_sum_logprobs,
                    length_penalty=length_penalty)
        import math
        results = [(tokenizer.decode(output.tolist()),math.exp(score.item()),beam_idx) for score, output,beam_idx in results]   
        return results

    def sampling_generate(self,instruction,input_text,token_count=128,
                          temperature=1.0,
                          top_p=0,
                          top_k=0,
                          alpha_frequency=0.25,
                          alpha_presence=0.25,
                          alpha_decay=0.996,
                          token_stop=[0,1]):
        self.set_adapter(self.chat_adapter_name)
        gen_args = PIPELINE_ARGS(temperature = temperature, top_p = top_p, top_k=top_k, # top_k = 0 then ignore
                        alpha_frequency = alpha_frequency,
                        alpha_presence = alpha_presence,
                        alpha_decay = alpha_decay, # gradually decay the penalty
                        token_ban = [], # ban the generation of some tokens
                        token_stop = [0,1], # stop generation whenever you see any token here
                        chunk_len = 512)
        cat_char = 'ðŸ±'
        bot_char = 'ðŸ¤–'
        ctx = f'{cat_char}:{instruction}\n{input_text}\n{bot_char}:'
        with torch.no_grad():
            with torch.autocast(enabled=True,device_type=self.device,dtype=self.dtype):
                output = generate(self.rwkv,ctx,self.tokenizer,token_count=token_count,args=gen_args,callback=None)
        return output
if __name__ == '__main__':
    base_rwkv_model = '/media/yueyulin/KINGSTON/models/rwkv6/RWKV-x060-World-1B6-v2.1-20240328-ctx4096.pth'
    bi_lora_path = '/media/yueyulin/data_4t/models/lora/biencoder/epoch_1_step_430000/RWKV-x060-World-1B6-v2_rwkv_lora.pth'
    cross_lora_path = '/media/yueyulin/data_4t/models/lora/cross_encoder/epoch_0_step_920000/RWKV-x060-World-1B6-v2_rwkv_lora.pth'
    tokenizer_file = os.path.join(parent_dir,'tokenizer','rwkv_vocab_v20230424.txt')
    from tokenizer.rwkv_tokenizer import TRIE_TOKENIZER
    tokenizer = TRIE_TOKENIZER(tokenizer_file)
    chat_lora_path = '/media/yueyulin/data_4t/models/pissa_r64/epoch_0/RWKV-x060-World-1B6-v2.1-20240328-ctx4096.pth.pth'
    chat_pissa_path = '/media/yueyulin/data_4t/models/pissa_r64/init_pissa.pth'
    chat_lora_r = 64
    chat_lora_alpha = 64
    fusedEncoder = BiCrossFusionEncoder(
        base_rwkv_model,
        bi_lora_path,
        cross_lora_path,
        chat_lora_path,
        tokenizer,
        chat_lora_r=chat_lora_r,
        chat_lora_alpha=chat_lora_alpha,
        chat_pissa_path=chat_pissa_path)

    texts = ['æˆ‘æ‰“ç®—å–æ¶ˆè®¢å•','æˆ‘è¦å–æ¶ˆè®¢å•','æˆ‘è¦é€€è´§','æˆ‘è¦é€€æ¬¾']
    outputs = fusedEncoder.encode_texts(texts)
    outputs = torch.tensor(outputs)
    print(outputs)
    from sentence_transformers.util import pairwise_cos_sim
    for qid in range(len(texts)):
        query = outputs[qid]
        for i in range(len(texts)):
            if i != qid:
                print(f'{texts[qid]} vs {texts[i]} is {pairwise_cos_sim(query.unsqueeze(0),outputs[i].unsqueeze(0))}')

        print('-----------------------')
    

    texts_a = ['FAQæ˜¯ä»€ä¹ˆï¼Ÿ','FAQæ˜¯ä»€ä¹ˆï¼Ÿ','FAQæ˜¯ä»€ä¹ˆï¼Ÿ','FAQæ˜¯ä»€ä¹ˆï¼Ÿ']
    texts_b = ['ä¸‹å›¾æ˜¯ç™¾åº¦ç™¾ç§‘å¯¹FAQçš„è§£é‡Šï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•çš„ç†è§£å…¶ä¸ºï¼Œç½‘ç«™ä¸­çš„å¸¸è§é—®é¢˜å¸®åŠ©ä¸­å¿ƒã€‚é‡‡ç”¨ä¸€é—®ä¸€ç­”çš„æ–¹å¼å¸®åŠ©å®¢æˆ·å¿«é€Ÿè§£å†³äº§å“/æœåŠ¡é—®é¢˜ï¼','ï¼šFAQ(Frequently Asked Questions)é—®ç­”ç³»ç»Ÿæ˜¯ç›®å‰åº”ç”¨æœ€å¹¿æ³›çš„é—®ç­”ç³»ç»Ÿã€‚è¿™ç§é—®ç­”ç³»ç»Ÿçš„ç»“æž„æ¡†æž¶æ˜Žäº†ã€å®žçŽ°ç®€å•ã€å®¹æ˜“ç†è§£ï¼Œéžå¸¸é€‚åˆä½œä¸ºé—®ç­”ç³»ç»Ÿå…¥é—¨å­¦ä¹ æ—¶çš„è§‚å¯Ÿå¯¹è±¡ã€‚è¿™é‡ŒåŸºäºŽæœ¬äººåœ¨é—®ç­”ç³»ç»Ÿå»ºè®¾æ–¹é¢çš„â€œå¤šå¹´â€ç»éªŒï¼Œå¯¹FAQé—®ç­”ç›¸å…³çš„å®šä¹‰ã€ç³»ç»Ÿç»“æž„ã€æ•°æ®é›†å»ºè®¾ã€å…³é”®æŠ€æœ¯ã€åº”ç”¨ç­‰æ–¹é¢è¿›è¡Œäº†æ•´ç†å’Œä»‹ç»ã€‚','FAQæ˜¯è‹±æ–‡ Frequently Asked Questionsçš„ç¼©å†™ã€‚ä¸­æ–‡æ„æ€æ˜¯â€œå¸¸è§é—®é¢˜â€ï¼Œæˆ–è€…æ›´é€šä¿—ç‚¹è¯´ï¼Œâ€œå¸¸è§é—®é¢˜è§£ç­”â€ã€‚FAQæ˜¯ç›®å‰äº’è”ç½‘ä¸Šæä¾›åœ¨çº¿å¸®åŠ©çš„ä¸»è¦æ–¹å¼ï¼Œé€šè¿‡äº‹å…ˆç»„ç»‡ä¸€äº›å¸¸è§çš„é—®ç­”ï¼Œåœ¨ç½‘é¡µä¸Šå‘å¸ƒå’¨è¯¢æœåŠ¡ã€‚','ä»ŽæŠ€æœ¯ï¼Œå³å®žçŽ°æ–¹å¼çš„è§’åº¦æ¥çœ‹ï¼Œé—®ç­”ç³»ç»Ÿæœ‰å¾ˆå¤šç§ï¼ŒåŒ…æ‹¬åŸºäºŽFAQçš„é—®ç­”ã€åŸºäºŽçŸ¥è¯†å›¾è°±çš„é—®ç­”ã€åŸºäºŽæ–‡æœ¬çš„é—®ç­”ç­‰ç­‰ã€‚è¿™é‡Œå›´ç»•åº”ç”¨æœ€ä¸ºå¹¿æ³›çš„FAQé—®ç­”ç³»ç»Ÿï¼Œå¯¹é—®ç­”ç³»ç»Ÿçš„å®šä¹‰ã€æ€æƒ³ã€åŸºæœ¬ç»“æž„ã€æ–¹æ³•å’Œåº”ç”¨ä»·å€¼è¿›è¡Œä»‹ç»ã€‚']
    outputs = fusedEncoder.cross_encode_texts(texts_a,texts_b)
    print(outputs)


    instruction ='æ ¹æ®ç»™å®šçš„çŸ­æ–‡ï¼Œå›žç­”ä»¥ä¸‹é—®é¢˜ï¼šé»„å¾ªè´¢çš„æ˜¯å“ªå›½äººï¼Ÿ'
    input_text = 'é»„å¾ªè´¢ï¼ˆè‹±è¯­ï¼šLawrence Wong Shyun Tsaiï¼Œ1972å¹´12æœˆ18æ—¥â€”ï¼‰ï¼Œæ–°åŠ å¡åŽè£”æ”¿æ²»äººç‰©ï¼ŒçŽ°ä»»æ–°åŠ å¡æ€»ç†å…¼è´¢æ”¿éƒ¨éƒ¨é•¿ã€äººæ°‘è¡ŒåŠ¨å…šç¤¾åŒºåŸºé‡‘ä¼šä¸»å¸­ã€‚ä»–ä¸ŽçŽ‹ä¹™åº·å’Œé¢œé‡‘å‹‡å…±åŒä¸»æŒäº†å› åº”æ–°åŠ å¡2019å† çŠ¶ç—…æ¯’ç—…å¤§æµè¡Œçš„å¤šéƒ¨å§”å·¥ä½œç»„ã€‚æ›¾ä»»æ–°åŠ å¡å‰¯æ€»ç†ï¼Œæ•™è‚²éƒ¨ã€å›½å®¶å‘å±•éƒ¨ã€æ–‡åŒ–ã€ç¤¾åŒºåŠé’å¹´éƒ¨çš„éƒ¨é•¿ï¼Œé€šè®¯åŠæ–°é—»éƒ¨å’Œè´¢æ”¿éƒ¨çš„ç¬¬äºŒéƒ¨é•¿ï¼Œä»¥åŠäººæ°‘è¡ŒåŠ¨å…šå‰¯ç§˜ä¹¦é•¿ã€‚[1]é»„å¾ªè´¢æ˜¯äººæ°‘è¡ŒåŠ¨å…šç¬¬å››ä»£é¢†å¯¼å±‚ï¼Œä¹Ÿæ˜¯äººæ°‘è¡ŒåŠ¨å…šä¸­å¤®æ‰§è¡Œå§”å‘˜ä¼šé¦–ä»»å‰¯ç§˜ä¹¦é•¿å…¼æ”¿ç­–è®ºå›é¡¾é—®ã€‚'
    output = fusedEncoder.sampling_generate(instruction,input_text)
    print(output)

    beam_results = fusedEncoder.beam_generate(instruction,input_text)
    for result in beam_results:
        print(result)