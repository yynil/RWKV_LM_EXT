from fastapi import FastAPI, Request,HTTPException
from pydantic import BaseModel
from typing import List, Optional
import time
import torch
import argparse
import json
from sentence_transformers.util import cos_sim as sim_fn
model = None
emb_model = None
tokenizer = None
states_runner = None
states_configuration = None
def setup_env():
    import os
    parent_path = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
    import sys
    sys.path.append(parent_path)
    print(f'add path: {parent_path} to sys.path')
    os.environ['RWKV_JIT_ON'] = '1'
    os.environ['RWKV_T_MAX'] = '4096'
    os.environ['RWKV_FLOAT_MODE'] = 'bf16'
    os.environ['RWKV_HEAD_SIZE_A'] = '64'
    os.environ['RWKV_T_MAX'] = '4096'
    os.environ["RWKV_MY_TESTING"]='x060'
    os.environ['RWKV_CTXLEN'] = '4096'
    os.environ['WKV'] = ''
    os.environ['RWKV_TRAIN_TYPE'] = ''
    os.environ['HF_ENDPOINT'] = "https://hf-mirror.com"
    os.environ['NO_CUDA'] = '1'
setup_env()

from transformers import AutoTokenizer
from src.model_encoder_run import RwkvEncoder
from src.model_run import create_empty_args,load_embedding_ckpt_and_parse_args
from infer.rwkv_states_runner import StatesRunner
def load_base_model(base_model):
    args = create_empty_args()
    w = load_embedding_ckpt_and_parse_args(base_model, args)
    print(args)
    args.emb_id = 151329
    args.pad_id = 151334
    args.mask_id = 151330
    model = RwkvEncoder(args)
    info = model.load_state_dict(w)
    print(info)
    return model
app = FastAPI()

class MaskFillResult(BaseModel):
    output_text: str
    score: float

class FillMaskResponse(BaseModel):
    results: Optional[List[MaskFillResult]] = None
    elapsed_time: str
    msg: Optional[str] = None
# å®šä¹‰è¯·æ±‚å’Œå“åº”æ¨¡å‹
class SentenceSimilarityRequest(BaseModel):
    input_text: str
    compared_texts: List[str]

class SentenceSimilarityResponse(BaseModel):
    similarities: Optional[List[float]] = None
    elapsed_time: str
    msg: Optional[str] = None

class InputData(BaseModel):
    input_text: str
    states_name: str

class OutputData(BaseModel):
    output_text: str
    elapsed_time: float

@app.post("/process_text", response_model=OutputData)
def process_text(data: InputData):
    start_time = time.time()
    
    # å‡è®¾states_runneræœ‰ä¸€ä¸ªprocessæ–¹æ³•æ¥å¤„ç†è¾“å…¥æ–‡æœ¬
    try:
        instruction = ''
        if data.states_name in states_configuration:
            instruction = states_configuration[data.states_name]['instruction']
        cat_char = 'ğŸ±'
        bot_char = 'ğŸ¤–'
        input_data = {'input':data.input_text}
        ctx = f'{cat_char}:{instruction}\n{json.dumps(input_data,ensure_ascii=False)}\n{bot_char}:'
        output_text = states_runner.generate(ctx,token_count=512,states_name=data.states_name)
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))
    
    elapsed_time = time.time() - start_time
    return OutputData(output_text=output_text, elapsed_time=elapsed_time)

def load_states_config(states_file_config):
    with open(states_file_config,'r') as f:
        return json.load(f)



# æ·»åŠ æ–°çš„è·¯ç”±å¤„ç†å‡½æ•°
@app.post("/compute_sentence_similarities")
async def compute_sentence_similarities(request: Request, similarity_request: SentenceSimilarityRequest):
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    # æ£€æŸ¥ input_text æ˜¯å¦ä¸ºå­—ç¬¦ä¸²
    if not isinstance(similarity_request.input_text, str):
        end_time = time.time()
        elapsed_time = f"{end_time - start_time:.4f}"
        return SentenceSimilarityResponse(
            similarities=None,
            elapsed_time=elapsed_time,
            msg="input_text å¿…é¡»æ˜¯å­—ç¬¦ä¸²"
        )

    # æ£€æŸ¥ compared_texts æ˜¯å¦ä¸ºå­—ç¬¦ä¸²æ•°ç»„
    if not isinstance(similarity_request.compared_texts, list) or not all(isinstance(text, str) for text in similarity_request.compared_texts):
        end_time = time.time()
        elapsed_time = f"{end_time - start_time:.4f}"
        return SentenceSimilarityResponse(
            similarities=None,
            elapsed_time=elapsed_time,
            msg="compared_texts å¿…é¡»æ˜¯å­—ç¬¦ä¸²æ•°ç»„"
        )

    # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ­¤å¤„ä¸ºç¤ºä¾‹ï¼Œå®é™…è®¡ç®—é€»è¾‘éœ€è¦æ ¹æ®å…·ä½“éœ€æ±‚å®ç°ï¼‰
    scores, elapsed_time = compute_similarities_internal([similarity_request.input_text] + similarity_request.compared_texts)

    return SentenceSimilarityResponse(similarities=scores, elapsed_time=elapsed_time)

@app.post("/fill_mask")
async def fill_mask(request: Request, input_text: str):
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    # è®°å½•è¯·æ±‚çš„IP
    client_ip = request.client.host
    print(f"Received request from IP: {client_ip}")

    # æ£€æŸ¥ input_text
    if len(input_text) > 128:
        end_time = time.time()
        elapsed_time = f"{end_time - start_time:.4f}"
        return FillMaskResponse(
            results=None,
            elapsed_time=elapsed_time,
            msg="è¾“å…¥æ–‡æœ¬é•¿åº¦ä¸èƒ½è¶…è¿‡128ä¸ªå­—ç¬¦"
        )

    mask_count = input_text.count("[MASK]")
    if mask_count != 1:
        end_time = time.time()
        elapsed_time = f"{end_time - start_time:.4f}"
        return FillMaskResponse(
            results=None,
            elapsed_time=elapsed_time,
            msg="è¾“å…¥æ–‡æœ¬å¿…é¡»åŒ…å«ä¸”ä»…åŒ…å«ä¸€ä¸ª[MASK]æ ‡è®°"
        )


    # è°ƒç”¨ fill_mask_internal å¤„ç†åˆæ³•è¾“å…¥
    internal_results = fill_mask_internal(input_text)
    # å°†å†…éƒ¨å¤„ç†ç»“æœè½¬æ¢ä¸º MaskFillResult å¯¹è±¡åˆ—è¡¨
    results = [MaskFillResult(output_text=result['output_text'], score=result['score']) for result in internal_results]
    
    # è®¡ç®—æ‰§è¡Œæ—¶é—´
    end_time = time.time()
    elapsed_time = f"{end_time - start_time:.4f}"
    
    return FillMaskResponse(results=results, elapsed_time=elapsed_time)

def fill_mask_internal(input_text):
    emb_id = 151329
    pad_id = 151334
    mask_id = 151330
    texts_idx = tokenizer.encode(input_text,add_special_tokens=False)
    texts_idx.append(emb_id)
    texts_idx = [texts_idx]
    mask_positions = []
    for text_idx in texts_idx:
        mask_positions.append([i for i, x in enumerate(text_idx) if x == args.mask_id])
    device = 'cpu'
    input_ids = torch.tensor(texts_idx,dtype=torch.long,device=device)
    MAX_CUM_PROB = 0.7
    import time
    results = []
    with torch.no_grad():
        with torch.autocast(device_type=device,dtype=torch.float32):
            logits = model.forward(input_ids)
            for b in range(len(texts_idx)):
                mask_position = mask_positions[b]
                masked_prob = torch.softmax(logits[b,mask_position],dim=-1)
                probs,indices = torch.topk(masked_prob,10)
                for position in mask_position:
                    cum_prob = 0
                    mask_idx = 0
                    for i in range(10):
                        texts_idx[b][position] = indices[mask_idx][i].item()
                        prob = probs[mask_idx][i].item()
                        cum_prob += prob
                        results.append({'output_text':tokenizer.decode(texts_idx[b],skip_special_tokens=True),'score':prob})
                        if cum_prob > MAX_CUM_PROB:
                            break
                    mask_idx += 1
    return results

def compute_similarities_internal(texts :List[str],device:str='cpu'):
    emb_id = 151329
    pad_id = 151334
    MAX_LEN = 4096
    texts_idx = [tokenizer.encode(text,add_special_tokens=False) for text in texts]
    texts_idx = [ text_idx[:MAX_LEN-1] + [emb_id] for text_idx in texts_idx]
    max_len = max([len(text_idx) for text_idx in texts_idx])
    texts_idx = [text_idx + [pad_id]*(max_len-len(text_idx)) for text_idx in texts_idx]
    input_ids = torch.tensor(texts_idx,dtype=torch.long,device=device)
    import time
    with torch.no_grad():
        with torch.autocast(device_type=device,dtype=torch.float32):
            start_time = time.time()
            embs = emb_model.encode_sentence(input_ids)
            end_time = time.time()
            scores = sim_fn(embs[0],embs[1:])
            return scores.squeeze(0).tolist(),f'{end_time-start_time:.4f}'


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser("Test MLM/Emb model")
    parser.add_argument("--model_file",type=str,default='/media/yueyulin/data_4t/models/mlm/final/epoch_0_step_100000/RWKV-x060-MLM-ctx4096.pth.pth')
    parser.add_argument("--emb_model_file",type=str,default='/media/yueyulin/KINGSTON/models/all_chinese_biencoder/trainable_model/epoch_9/RWKV-x060-MLM-ctx4096.pth.pth')
    parser.add_argument("--device",type=str,default='cuda:0')
    parser.add_argument('--dtype',type=str,default='fp16',choices=['fp16','fp32','bf16'])
    parser.add_argument('--states_file_config',type=str)
    parser.add_argument('--llm_model_file',type=str,default='/media/yueyulin/KINGSTON/models/rwkv6/RWKV-x060-World-7B-v2.1-20240507-ctx4096.pth')
    args = parser.parse_args() 
    print(args)
    ###Init llm
    strategy = f'{args.device} {args.dtype}'
    if args.dtype == 'fp16':
        dtype = torch.half
    elif args.dtype == 'fp32':
        dtype = torch.float32
    else:
        dtype = torch.bfloat16
    states_runner = StatesRunner(args.llm_model_file,strategy,args.device,dtype)
    states_configuration = load_states_config(args.states_file_config)
    for states_name in states_configuration.keys():
        states_file = states_configuration[states_name]['file']
        states_runner.add_states(states_name,states_file)
    cat_char = 'ğŸ±'
    bot_char = 'ğŸ¤–'
    instruction ='æ ¹æ®inputä¸­æ–‡æœ¬å†…å®¹ï¼ŒååŠ©ç”¨æˆ·è¯†åˆ«æ–‡æœ¬æ‰€å±çš„é¢†åŸŸã€‚éšåï¼Œæ‰¾å‡ºä¸è¯¥é¢†åŸŸå…³è”æœ€ç´§å¯†çš„ä¸“å®¶ã€‚æ¥ç€ï¼Œä½œä¸ºè¾“å‡ºï¼Œåˆ—ä¸¾å‡ºäº”è‡³åé¡¹å¯åœ¨è¯¥æ–‡æœ¬ä¸­æ‰§è¡Œçš„å…·ä½“ä»»åŠ¡ã€‚æ¥ä¸‹æ¥ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼šé¢†åŸŸï¼šå¯¹äºç»™å®šçš„ç¤ºä¾‹æ–‡æœ¬ï¼Œå¸®åŠ©ç”¨æˆ·æŒ‡å®šä¸€ä¸ªæè¿°æ€§é¢†åŸŸï¼Œæ¦‚æ‹¬æ–‡æœ¬çš„ä¸»é¢˜ã€‚è¯·æŒ‰ç…§JSONå­—ç¬¦ä¸²çš„æ ¼å¼å›ç­”ï¼Œæ— æ³•æå–åˆ™ä¸è¾“å‡º'
    input_text = '{\"input\":\"è¶…é•¿æœŸç‰¹åˆ«å›½å€ºï¼ˆultra-long special treasury bondsï¼‰ï¼Œä¸€èˆ¬æŒ‡å‘è¡ŒæœŸé™åœ¨10å¹´ä»¥ä¸Šçš„ï¼Œä¸ºç‰¹å®šç›®æ ‡å‘è¡Œçš„ã€å…·æœ‰æ˜ç¡®ç”¨é€”çš„å›½å€ºã€‚è¶…é•¿æœŸç‰¹åˆ«å›½å€ºä¸“é¡¹ç”¨äºå›½å®¶é‡å¤§æˆ˜ç•¥å®æ–½å’Œé‡ç‚¹é¢†åŸŸå®‰å…¨èƒ½åŠ›å»ºè®¾ï¼Œ2024å¹´å…ˆå‘è¡Œ1ä¸‡äº¿å…ƒï¼ŒæœŸé™åˆ†åˆ«ä¸º20å¹´ã€30å¹´ã€50å¹´ã€‚ [1]\
    2024å¹´5æœˆ13æ—¥ï¼Œè´¢æ”¿éƒ¨ç½‘ç«™å…¬å¸ƒ2024å¹´ä¸€èˆ¬å›½å€ºã€è¶…é•¿æœŸç‰¹åˆ«å›½å€ºå‘è¡Œæœ‰å…³å®‰æ’ã€‚ [6-7]2024å¹´5æœˆ17æ—¥ï¼Œ30å¹´æœŸè¶…é•¿æœŸç‰¹åˆ«å›½å€ºæ­£å¼é¦–å‘ã€‚æ ¹æ®å‘è¡Œå®‰æ’ï¼Œé¦–å‘çš„30å¹´æœŸè¶…é•¿æœŸç‰¹åˆ«å›½å€ºï¼Œä¸ºå›ºå®šåˆ©ç‡é™„æ¯å€ºï¼Œæ€»é¢400äº¿å…ƒã€‚ [8]6æœˆ14æ—¥,è´¢æ”¿éƒ¨å‘è¡Œ2024å¹´è¶…é•¿æœŸç‰¹åˆ«å›½å€ºï¼ˆä¸‰æœŸï¼‰ï¼ˆ50å¹´æœŸï¼‰ï¼Œç«äº‰æ€§æ‹›æ ‡é¢å€¼æ€»é¢350äº¿å…ƒã€‚ [13]7æœˆ24æ—¥ï¼Œé€šè¿‡è´¢æ”¿éƒ¨æ”¿åºœå€ºåˆ¸å‘è¡Œç³»ç»Ÿæ‹›æ ‡å‘è¡Œ550äº¿å…ƒ30å¹´æœŸè¶…é•¿æœŸç‰¹åˆ«å›½å€ºï¼Œç¥¨é¢åˆ©ç‡åœ¨å½“å¤©é€šè¿‡ç«äº‰æ€§æ‹›æ ‡ç¡®å®šã€‚ [15]\"}'
    ctx = f'{cat_char}:{instruction}\n{input_text}\n{bot_char}:'
    print(ctx)
    print('start to generate...')
    output = states_runner.generate(ctx,token_count=512,states_name='domain_expert')
    print(output)
    ###
    model = load_base_model(args.model_file)
    emb_model = load_base_model(args.emb_model_file)
    tokenizer = AutoTokenizer.from_pretrained('THUDM/glm-4-9b-chat', trust_remote_code=True)
    args.emb_id = 151329
    args.pad_id = 151334
    args.mask_id = 151330
    dtype = torch.float32
    model = model.to(dtype=dtype)
    print(model)
    input_text = 'ä½ æ˜¯å¤©è¾¹çš„äº‘å½©ï¼Œæˆ‘æ˜¯[MASK]çš„é£ç­ã€‚'
    results = fill_mask_internal(input_text)
    print(results)
    texts = ['æ¯å¤©åƒè‹¹æœæœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ',
             'å®ç¥å®‰çœ ï¼šè‹¹æœä¸­å«æœ‰çš„ç£·å’Œé“ç­‰å…ƒç´ ï¼Œæ˜“è¢«è‚ å£å¸æ”¶ï¼Œæœ‰è¡¥è„‘å…»è¡€ã€å®ç¥å®‰çœ ä½œç”¨ã€‚è‹¹æœçš„é¦™æ°”æ˜¯æ²»ç–—æŠ‘éƒå’Œå‹æŠ‘æ„Ÿçš„è‰¯è¯ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨è¯¸å¤šæ°”å‘³ä¸­ï¼Œè‹¹æœçš„é¦™æ°”å¯¹äººçš„å¿ƒç†å½±å“æœ€å¤§ï¼Œå®ƒå…·æœ‰æ˜æ˜¾çš„æ¶ˆé™¤å¿ƒç†å‹æŠ‘æ„Ÿçš„ä½œç”¨ã€‚',
             'ç¾ç™½å…»é¢œã€é™ä½èƒ†å›ºé†‡ï¼šè‹¹æœä¸­çš„èƒ¶è´¨å’Œå¾®é‡å…ƒç´ é“¬èƒ½ä¿æŒè¡€ç³–çš„ç¨³å®šï¼Œè¿˜èƒ½æœ‰æ•ˆåœ°é™ä½èƒ†å›ºé†‡ã€‚è‹¹æœä¸­çš„ç²—çº¤ç»´å¯ä¿ƒè¿›è‚ èƒƒè •åŠŸï¼Œå¹¶å¯Œå«é“ã€é”Œç­‰å¾®é‡å…ƒç´ ï¼Œå¯ä½¿çš®è‚¤ç»†æ¶¦æœ‰å…‰æ³½ï¼Œèµ·åˆ°ç¾å®¹ç˜¦èº«çš„ä½œç”¨ã€‚',
             'è‹¹æœç”Ÿåƒæ²»ä¾¿ç§˜ï¼Œç†Ÿåƒæ²»è…¹æ³»ï¼šè‹¹æœä¸­å«æœ‰ä¸°å¯Œçš„é£é…¸ã€æœèƒ¶ã€è†³é£Ÿçº¤ç»´ç­‰ç‰¹æ®Šç‰©è´¨ï¼Œé£é…¸æ˜¯è‚ é“æ”¶æ•›å‰‚ï¼Œå®ƒèƒ½å‡å°‘è‚ é“åˆ†æ³Œè€Œä½¿å¤§ä¾¿å†…æ°´åˆ†å‡å°‘ï¼Œä»è€Œæ­¢æ³»ã€‚è€Œæœèƒ¶åˆ™æ˜¯ä¸ªâ€œä¸¤é¢æ´¾â€ï¼Œæœªç»åŠ çƒ­çš„ç”Ÿæœèƒ¶æœ‰è½¯åŒ–å¤§ä¾¿ç¼“è§£ä¾¿ç§˜çš„ä½œç”¨ï¼Œç…®è¿‡çš„æœèƒ¶å´æ‘‡èº«ä¸€å˜ï¼Œå…·æœ‰æ”¶æ•›ã€æ­¢æ³»çš„åŠŸæ•ˆã€‚è†³é£Ÿçº¤ç»´åˆèµ·åˆ°é€šä¾¿ä½œç”¨ã€‚',
             'ä¿æŠ¤å¿ƒè„ï¼šè‹¹æœçš„çº¤ç»´ã€æœèƒ¶ã€æŠ—æ°§åŒ–ç‰©ç­‰èƒ½é™ä½ä½“å†…åèƒ†å›ºé†‡å¹¶æé«˜å¥½èƒ†å›ºé†‡å«é‡ï¼Œæ‰€ä»¥æ¯å¤©åƒä¸€ä¸¤ä¸ªè‹¹æœä¸å®¹æ˜“å¾—å¿ƒè„ç—…ã€‚']
    scores,elapsed_time = compute_similarities_internal(texts)
    print(scores)
    print(elapsed_time)
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)